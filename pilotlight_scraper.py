#!/usr/bin/env python3
# ============================================================================
# Script:       pilotlight_scraper.py
# Version:      2.0.0
# Date:         2026-02-17
# Purpose:      Scrape Pilot Light upcoming events, output one artist per CSV row
#
# Scrapes thepilotlight.com for upcoming show listings and writes a CSV with
# columns: date, artist, price.  Each artist on a multi-band bill gets its
# own row so downstream tools can look up tracks per artist.
#
# Usage:        python pilotlight_scraper.py [OPTIONS]
# Input:        None (fetches from thepilotlight.com)
# Output:       CSV file with date,artist,price columns
#
# Options:
#   --week_start N   Weeks from today to start (default: 0)
#   --week_stop  M   Weeks from today to stop  (default: -1, no limit)
#   --filename FILE  Output CSV filename
#   -v, --verbose    Enable debug output
#   -h, --help       Show this help message
#
# Requirements:
#   - requests, beautifulsoup4 in PATH/venv
#
# Session:      PilotLight Playlist Fixes
# AI Model:     Claude Opus 4.6
# Attribution:  Generated by Michael Gilchrist in collaboration with
#               ClaudeAI Opus 4.6
# ============================================================================

import argparse
import csv
import logging
import re
import sys
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import requests
from bs4 import BeautifulSoup

log = logging.getLogger(__name__)

# Phrases in event text that are not band names
IGNORE_PHRASES = {"OUT SERIES", "UPDATE", "UPDATE!", "IMPROVISED MUSIC SETS"}

URL = "https://thepilotlight.com"


def _parse_single_event(event_text, today, start_date, end_date):
    """Parse one event's text block into (date, artist, price) tuples.

    Returns an empty list if the text does not contain a recognizable event.
    """
    # Normalize spaces around dollar signs (site renders "$1 0" or "$ 10" for "$10")
    normalized = re.sub(r"\$\s+", "$", event_text)       # "$ 10" -> "$10"
    normalized = re.sub(r"\$(\d)\s+(\d)", r"$\1\2", normalized)  # "$1 0" -> "$10"

    # Match pattern: Weekday Month Day ... $PRICE
    m = re.search(
        r"(\w+day)\s+([A-Za-z]+)\s+(\d+)\w*\s.*?\$(FREE|\d+)",
        normalized,
        flags=re.IGNORECASE,
    )
    if not m:
        return []

    month_str, day_str, price = m.group(2), m.group(3), m.group(4).upper()

    # Parse the show date, trying abbreviated and full month names
    date_text = f"{month_str} {day_str} {today.year}"
    show_date = None
    for fmt in ("%b %d %Y", "%B %d %Y"):
        try:
            show_date = datetime.strptime(date_text, fmt).date()
            break
        except ValueError:
            continue
    if show_date is None:
        return []

    # Year-boundary fix: if parsed date is >6 months in the past,
    # assume it belongs to next year (e.g., scraping Dec for Jan shows)
    six_months_ago = today - relativedelta(months=6)
    if show_date < six_months_ago:
        show_date = show_date.replace(year=today.year + 1)

    # Apply date filters
    if show_date < start_date or (end_date and show_date > end_date):
        return []

    # Extract band names: text after pricing info up to "Get your" or end
    after_price = normalized[m.end():]
    # Skip "advance / $20 day of show" dual-pricing patterns
    after_price = re.sub(
        r"^\s*advance\s*/\s*\$\d+\s*day\s+of\s+show\s*",
        " ", after_price, flags=re.IGNORECASE)
    # Strip ticket link text, "Out Series" descriptions, and trailing noise
    after_price = re.split(r"Get your|Advance Tickets|Out Series",
                           after_price, flags=re.IGNORECASE)[0]

    bands = []
    # Split on common delimiters between band names
    for part in re.split(r"\bwith\b|,|/|\band\b|\+", after_price,
                         flags=re.IGNORECASE):
        part = part.strip()
        if len(part) < 2:
            continue
        # Band names are ALL CAPS on this site.  Accept if:
        #   - has at least one letter
        #   - all ASCII letters are uppercase
        #   - not a pure number or price fragment
        letters = [c for c in part if c.isascii() and c.isalpha()]
        if not letters:
            continue
        if not all(c.isupper() for c in letters):
            continue
        bands.append(part)

    if not bands:
        return []

    log.debug("Parsed event: %s | %s | $%s", show_date, ", ".join(bands), price)
    return [(str(show_date), band, price) for band in bands]


def _parse_events_from_tag(p_tag, today, start_date, end_date):
    """Parse a <p> tag that may contain one or multiple events.

    The website sometimes packs multiple events into a single <p> tag.
    We split on weekday names to isolate individual events first.
    """
    # Remove struck-through text (cancelled acts)
    for strike in p_tag.find_all("s"):
        strike.decompose()

    text = p_tag.get_text(separator=" ")
    text = text.replace("\u2013", "-").replace("\u2014", "-")
    text = re.sub(r"[ \t]+", " ", text)

    # Split on weekday boundaries (lookahead so we keep the weekday name)
    event_chunks = re.split(
        r"(?=(?:Mon|Tues|Wednes|Thurs|Fri|Satur|Sun)day\s+[A-Za-z]+\s+\d+)",
        text,
    )

    rows = []
    for chunk in event_chunks:
        chunk = chunk.strip()
        if not chunk:
            continue
        rows.extend(_parse_single_event(chunk, today, start_date, end_date))

    return rows


def scrape_upcoming_artists(output_path=None, week_start=0, week_stop=-1,
                            verbose=False):
    """Scrape thepilotlight.com and write one-artist-per-row CSV.

    Returns the output file path.
    """
    if verbose and not log.handlers:
        logging.basicConfig(format="%(message)s", level=logging.DEBUG)

    today = datetime.today().date()
    start_date = today + timedelta(weeks=week_start)
    end_date = None if week_stop < 0 else today + timedelta(weeks=week_stop)

    try:
        resp = requests.get(URL, timeout=30)
        resp.raise_for_status()
        log.debug("Fetched %s (%d bytes, status %d)",
                  URL, len(resp.text), resp.status_code)
    except Exception as e:
        log.error("Failed to fetch site: %s", e)
        sys.exit(1)

    soup = BeautifulSoup(resp.text, "html.parser")
    rows = []
    for p in soup.find_all("p"):
        rows.extend(_parse_events_from_tag(p, today, start_date, end_date))

    if not output_path:
        stamp = datetime.now().strftime("%Y-%m-%d_%H%M")
        output_path = f"upcoming-artists_{stamp}.csv"

    with open(output_path, "w", newline="", encoding="utf-8") as fp:
        writer = csv.writer(fp)
        writer.writerow(["date", "artist", "price"])
        writer.writerows(rows)

    log.info("Wrote %d artist rows -> %s", len(rows), output_path)

    if week_stop < 0:
        log.info("Suggested playlist title: Upcoming Shows")
    else:
        start_lbl = start_date.strftime("%b %d")
        end_lbl = (end_date - timedelta(days=1)).strftime("%b %d")
        log.info("Suggested playlist title: Pilot Light Shows %s-%s",
                 start_lbl, end_lbl)

    return output_path


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Scrape Pilot Light upcoming events")
    parser.add_argument("--week_start", type=int, default=0,
                        help="Weeks from today to start")
    parser.add_argument("--week_stop", type=int, default=-1,
                        help="Weeks to stop (no limit if negative)")
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="Enable debug output")
    parser.add_argument("--filename", type=str,
                        help="Output CSV filename")
    args = parser.parse_args()

    scrape_upcoming_artists(
        output_path=args.filename,
        week_start=args.week_start,
        week_stop=args.week_stop,
        verbose=args.verbose,
    )
