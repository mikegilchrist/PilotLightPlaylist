#!/usr/bin/env python3
# ============================================================================
# Script:       create_pilotlight_playlist.py
# Version:      2.0.0
# Date:         2026-02-17
# Purpose:      Scrape Pilot Light events and update Spotify playlists
#
# Scrapes upcoming show listings, splits artists into "upcoming" (future
# shows) and "recent" (past shows), then updates the corresponding Spotify
# playlists with each artist's top tracks.
#
# Usage:        python create_pilotlight_playlist.py [OPTIONS] [filename]
# Input:
#   filename          Optional pre-scraped artist CSV (date,artist,price)
#   --backup-only     Only back up playlists, don't update
#   --dry-run         Write to ./tmp-* files, don't update Spotify
#   --force / -f      Overwrite existing backup files
#   --verbose         Print progress messages
#   --debug           Write detailed log to logs/ and enable extra verbosity
# Output:       Updates Spotify playlists or writes dry-run output files
#
# Requirements:
#   - requests, beautifulsoup4, spotipy, python-dotenv, pyyaml,
#     python-dateutil
#
# Session:      PilotLight Playlist Fixes
# AI Model:     Claude Opus 4.6
# Attribution:  Generated by Michael Gilchrist in collaboration with
#               ClaudeAI Opus 4.6
# ============================================================================

import argparse
import csv
import logging
import os
import sys
from datetime import datetime

from pilotlight_scraper import scrape_upcoming_artists
from playlist_utils import backup_playlist, init_spotify, load_config, update_playlist

log = logging.getLogger("pilotlight")


def setup_logging(debug=False, verbose=False):
    """Configure console and optional file logging."""
    # Set our loggers to desired level; silence noisy third-party loggers
    level = (logging.DEBUG if debug else
             logging.INFO if verbose else
             logging.WARNING)

    # Our project loggers
    for name in ("pilotlight", "pilotlight_scraper", "playlist_utils"):
        logging.getLogger(name).setLevel(level)

    # Quiet third-party HTTP/API noise even in debug mode
    for name in ("urllib3", "spotipy", "requests"):
        logging.getLogger(name).setLevel(logging.WARNING)

    root = logging.getLogger()
    root.setLevel(level)

    # Console handler
    console = logging.StreamHandler()
    console.setLevel(level)
    console.setFormatter(logging.Formatter("%(message)s"))
    root.addHandler(console)

    # File handler (debug mode only)
    if debug:
        os.makedirs("logs", exist_ok=True)
        stamp = datetime.now().strftime("%Y-%m-%d_%H%M")
        log_path = f"logs/debug_{stamp}.log"
        fh = logging.FileHandler(log_path, encoding="utf-8")
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(logging.Formatter(
            "%(asctime)s %(name)s %(levelname)s %(message)s"))
        root.addHandler(fh)
        log.info("Debug log -> %s", log_path)


def parse_args():
    parser = argparse.ArgumentParser(
        description="Update Spotify playlists from Pilot Light show listings.")
    parser.add_argument(
        "filename", nargs="?",
        help="Optional pre-scraped artist CSV (date,artist,price)")
    parser.add_argument(
        "--backup-only", action="store_true",
        help="Only back up playlists, don't update")
    parser.add_argument(
        "--dry-run", action="store_true",
        help="Simulate update, write to ./tmp-* files")
    parser.add_argument(
        "--force", "-f", action="store_true",
        help="Overwrite existing backup files")
    parser.add_argument(
        "--verbose", action="store_true",
        help="Print detailed progress")
    parser.add_argument(
        "--debug", action="store_true",
        help="Write detailed log to logs/ with extra verbosity")
    return parser.parse_args()


def read_artist_csv(filepath):
    """Read a scraped artist CSV and return list of dicts with
    keys: date, artist, price."""
    rows = []
    with open(filepath, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    return rows


def split_by_date(artist_rows, today):
    """Split artist rows into upcoming (>= today) and recent (< today)."""
    upcoming = []
    recent = []
    for row in artist_rows:
        try:
            show_date = datetime.strptime(row["date"], "%Y-%m-%d").date()
        except (ValueError, KeyError):
            log.warning("Skipping row with bad date: %s", row)
            continue
        if show_date >= today:
            upcoming.append(row)
        else:
            recent.append(row)
    return upcoming, recent


def make_backup_path(key, now_str, force=False):
    """Generate a backup filename, avoiding overwrites unless forced."""
    backup_file = f"playlist_{key}-artists_{now_str}.csv"
    if os.path.exists(backup_file) and not force:
        base, ext = os.path.splitext(backup_file)
        counter = 1
        while os.path.exists(f"{base}-{counter}{ext}"):
            counter += 1
        backup_file = f"{base}-{counter}{ext}"
    return backup_file


def main():
    args = parse_args()

    # --debug implies --verbose
    if args.debug:
        args.verbose = True
    setup_logging(debug=args.debug, verbose=args.verbose)

    config = load_config()
    now_dt = datetime.now()
    now_str = now_dt.strftime("%Y-%m-%d_%H%M")
    date_str = now_dt.strftime("%Y-%m-%d")
    today = now_dt.date()

    # ---- Determine / generate the scraped artist file ----
    if not args.filename:
        args.filename = f"upcoming-artists_{date_str}.csv"

    if args.dry_run:
        scrape_path = f"./tmp-{os.path.basename(args.filename)}"
    else:
        scrape_path = args.filename

    if not os.path.exists(scrape_path):
        if args.verbose:
            print(f"[INFO] Scraping and saving to {scrape_path}")
        scrape_upcoming_artists(output_path=scrape_path, verbose=args.verbose)
    else:
        if args.verbose:
            print(f"[INFO] Using existing scraped file: {scrape_path}")

    # ---- Authenticate with Spotify ----
    sp = init_spotify()

    # ---- Back up current playlists ----
    for key in ["upcoming", "recent"]:
        playlist_cfg = config["playlists"].get(key)
        if not playlist_cfg:
            continue

        backup_file = make_backup_path(key, now_str, force=args.force)
        if args.dry_run:
            backup_file = f"./tmp-{os.path.basename(backup_file)}"

        if args.verbose:
            prefix = "[DRY RUN] " if args.dry_run else ""
            print(f"{prefix}Backing up: {playlist_cfg['name']} -> {backup_file}")

        backup_playlist(sp, playlist_cfg["id"], backup_file)

    if args.backup_only:
        print("Backup complete.")
        sys.exit(0)

    # ---- Read scraped artist data and split by date ----
    all_artists = read_artist_csv(scrape_path)
    upcoming_artists, recent_artists = split_by_date(all_artists, today)

    log.info("Artists split: %d upcoming, %d recent",
             len(upcoming_artists), len(recent_artists))

    # ---- Update each playlist with its appropriate artist set ----
    playlist_data = {
        "upcoming": upcoming_artists,
        "recent": recent_artists,
    }

    for key, playlist_cfg in config["playlists"].items():
        artists = playlist_data.get(key, [])
        if not artists:
            if args.verbose:
                print(f"[INFO] No artists for '{key}' playlist, skipping update")
            continue

        if args.dry_run:
            output_file = f"./tmp-playlist_{key}-artists_{now_str}.csv"
        else:
            output_file = f"playlist_{key}-artists_{now_str}.csv"

        update_playlist(
            sp,
            artists,
            playlist_cfg,
            output_file,
            dry_run=args.dry_run,
            verbose=args.verbose,
        )


if __name__ == "__main__":
    main()
